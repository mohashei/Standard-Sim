import os
import cv2
import json
import random
import imageio
import collections
import numpy as np
from PIL import Image, ImageDraw
from pathlib import Path

import torch

from synthetic_data_baselines.datasets.base_dataset import BaseDataset
from .augment import (
    add_noise_instance,
    flip_lr_instance,
    mask2bbox,
)


class SimpleInstanceSegDataSet(BaseDataset):
    """Dataset of synthetic image pairs and change masks from synthetic images generated by blender
    Using official splits from imagesets dir.
    """

    DELIM_CHAR = ["-", "_"]
    IMG_FILE1 = "0.png"  # _1.png
    IMG_FILE2 = "1.png"  # _2.png
    LABEL_FILE = "_label.png"
    BBOX_FILE = "_bbox.json"

    def __init__(self, configuration):
        self.image_root = configuration["image_root"]
        self.json_root = configuration["json_root"]
        self.split = configuration["mode"]
        assert self.split in ["train", "val", "test"]
        self.files = collections.defaultdict(list)
        self.img_normalize = configuration["normalize"]
        self.crops = configuration["crop"]
        self.resize = configuration["resize"]
        self.spatial_resolution = configuration["spatial_resolution"]

        if not Path('/root/.imageio/freeimage').exists():
            imageio.plugins.freeimage.download()

        # For overfitting always use first 4 samples of train set
        if configuration["overfit"]:
            print("Overfitting to 4 samples")
            with open(os.path.join('/app/synthetic_data_baselines/imagesets/train.txt'), 'r') as imgsetFile:
                scenes = imgsetFile.readlines()
            scenes = [s[:-1] for s in scenes]
            scenes.sort()
            scenes = scenes[:4]
        else:
            with open(os.path.join('/app/synthetic_data_baselines/imagesets', self.split+'.txt'), 'r') as imgsetFile:
                scenes = imgsetFile.readlines()
            scenes = [s[:-1] for s in scenes]
            #examples_png = ["/app/renders_multicam_diff_1/"+e[:-1]+"_change-0.png" for e in examples_png]

        # REMOVE AFTER DEBUG
        scenes.sort()
        scenes = scenes[:5]
        #scenes = ['stcg.us.0003_6167664672e749bb8f088c60fad9a9ad_cam-2', 'cmps.ca.0000_117bf38b11904501b45519ec635a1278_cam-2', 'cmps.ca.0000_48a1a13a92264a2bbfae862347ebeb58_cam-2', 'stcg.us.0003_8fde099874d74ae8bfbab10413ca8d09_cam-0', 'cmps.ca.0000_1b67a22fa28b4bf29ed934d18faaad38_cam-2', 'stcg.us.0003_4992990beb4646e0b6695e4d42bc2615_cam-2', 'circ.us.0000_fa64b0e57a7249b18e2bb4b77ab37388_cam-0', 'cmps.ca.0000_e2b2f882abf148eb993c0a228c50c5a5_cam-1']
        
        for scene in scenes:
            json_anno_change_0 = os.path.join(self.json_root, scene+"_change-0.json")
            with open(json_anno_change_0, 'r') as jsonFile0:
                change_0_info = json.load(jsonFile0)
            self.files[self.split].append(
                {
                    "img": change_0_info["name"],
                    "bbox": change_0_info["bbox"],
                    "masks": change_0_info["masks"],
                    "ids": change_0_info["ids"]
                }
            )

            json_anno_change_1 = os.path.join(self.json_root, scene+"_change-1.json")
            with open(json_anno_change_1, 'r') as jsonFile1:
                change_1_info = json.load(jsonFile1)
            self.files[self.split].append(
                {
                    "img": change_1_info["name"],
                    "bbox": change_1_info["bbox"],
                    "masks": change_1_info["masks"],
                    "ids": change_1_info["ids"]
                }
            )
            

        self.mean = torch.tensor([0.406 * 255, 0.456 * 255, 0.485 * 255]).view(3, 1, 1)
        self.std = torch.tensor([0.225 * 255, 0.224 * 255, 0.229 * 255]).view(3, 1, 1)
        self.mean_depth = torch.tensor(7.7304)
        self.depth_std = torch.tensor(1.4214)

        self.res_x = self.spatial_resolution[0]
        self.res_y = self.spatial_resolution[1]

        self.aug = configuration["augment"]

        self.augment_params = {
            "box_augment_params": {
                "max_boxes": 3,
                "min_height_mult": 0.1,
                "max_height_mult": 0.5,
                "min_width_mult": 0.1,
                "max_width_mult": 0.3,
                "sat_prob": 0.5,
                "sat_min": 0.5,
                "sat_max": 1.5,
                "brightness_prob": 0.5,
                "brightness_min": 0.5,
                "brightness_max": 1.5,
            },
            "noise_params": {
                "qe_low": 0.65,
                "qe_high": 0.72,
                "bit_depth": 8,
                "baseline": 0,
                "sensitivity_low": 1.2,
                "sensitivity_high": 1.7,
                "dark_noise_low": 2.5,
                "dark_noise_high": 3.5,
            },
            "flip_pair": 0.0,
            "flip_lr": 0.5,
        }


    def __len__(self):
        return len(self.files[self.split])

    def augment(
        self, img, mask, boxes,
    ):
        img = add_noise_instance(img, self.augment_params["noise_params"])
        if random.random() < self.augment_params["flip_lr"]:
            img, mask, boxes = flip_lr_instance(img, mask, boxes)
        return img, mask, boxes

    def resize_imgs(
        self, img, mask, boxes
    ):
        height, width, _ = img.shape
        img = cv2.resize(img, (self.res_x, self.res_y))
        mask = cv2.resize(mask, (self.res_x, self.res_y), interpolation=cv2.INTER_NEAREST)
        x_scale = self.res_x / width
        y_scale = self.res_y / height
        resized_boxes = []
        for box in boxes:
            (origMinRow, origMinCol, origMaxRow, origMaxCol) = box
            y = int(np.round(origMinRow * y_scale))
            x = int(np.round(origMinCol * x_scale))
            ymax = int(np.round(origMaxRow * y_scale))
            xmax = int(np.round(origMaxCol * x_scale))
            resized_boxes.append([y, x, ymax, xmax])
        return img, mask, np.array(resized_boxes)

    def crop(
        self, img, mask, boxes, ids,
    ):
        height, width = img.shape[:2]

        bbox = mask2bbox(mask)
        if boxes is not None:
            change_rmin, change_cmin, change_rmax, change_cmax = bbox
        else:
            change_rmin, change_cmin, change_rmax, change_cmax = 0, 0, height, width

        bbheight = change_rmax - change_rmin
        bbwidth = change_cmax - change_cmin

        if bbheight < self.res_y:
            rmin = max(change_rmin - self.res_y + bbheight, 0)
            rmax = change_rmin
        else:
            rmin = change_rmin
            rmax = change_rmin - self.res_y + bbheight
        if bbwidth < self.res_x:
            cmin = max(change_cmin - self.res_x + bbwidth, 0)
            cmax = change_cmin
        else:
            cmin = change_cmin
            cmax = change_cmin - self.res_x + bbwidth

        x = random.randint(cmin, cmax)
        y = random.randint(rmin, rmax)
        x = min(x, width - self.res_x - 1)
        y = min(y, height - self.res_y - 1)
        ymax = y + self.res_y
        xmax = x + self.res_x
        img = img[y:ymax, x:xmax, :]
        mask = mask[y:ymax, x:xmax]

        cropped_boxes, new_ids = [], []
        for i in range(len(boxes)):
            box = boxes[0]

            if box[0] < y:
                new_y1 = y
            elif box[0] > ymax:
                new_y1 = ymax
            else:
                new_y1 = box[0]

            if box[1] < x:
                new_x1 = x
            elif box[1] > xmax:
                new_x1 = xmax
            else:
                new_x1 = box[1]

            if box[2] < y:
                new_y2 = y
            elif box[2] > ymax:
                new_y2 = ymax
            else:
                new_y2 = box[2]

            if box[3] < x:
                new_x2 = x
            elif box[3] > xmax:
                new_x2 = xmax
            else:
                new_x2 = box[3]

            cropped = [new_y1, new_x1, new_y2, new_x2]
            if cropped[0] == cropped[2] or cropped[1] == cropped[3]:
                continue
            else:
                cropped_boxes.append(cropped)
                new_ids.append(ids[i])
        return img, mask, np.array(cropped_boxes), np.array(new_ids)

    def get_store(self, index: int):
        store = os.path.basename(self.files[self.split][index]["img"]).split("_")[0]
        return store

    def __getitem__(self, index: int):
        datafiles = self.files[self.split][index]

        # Image
        img_file = datafiles["img"]
        img = cv2.imread(img_file)
        height, width, _ = img.shape
        scene = img_file.split('/')[-1][:-4]

        # Bounding box
        boxes = datafiles["bbox"]

        # Masks
        polygons = datafiles["masks"]
        ids = np.array(datafiles["ids"])
        mask = np.zeros((height, width))
        for i in range(len(polygons)):
            poly_mask = Image.new('L', (width, height), 0)
            poly = [(e[1], e[0]) for e in polygons[i]]
            ImageDraw.Draw(poly_mask).polygon(poly, outline=1, fill=1)
            poly_mask = np.array(poly_mask)
            mask[poly_mask == 1] = i+1
        
        if self.crops and not self.resize:
            img, mask, boxes, ids = self.crop(img, mask, boxes, ids)
        elif self.resize and not self.crops:
            img, mask, boxes = self.resize_imgs(img, mask, boxes)
        elif self.crops and self.resize:
            if random.random() > 0.5:
                img, mask, boxes = self.resize_imgs(img, mask, boxes)
            else:
                img, mask, boxes, ids = self.crop(img, mask, boxes, ids)
        if self.aug:
            img, mask, boxes = self.augment(img, mask, boxes)
        '''
        # Save mask image for debug visualization
        debug_path = '/app/synthetic_data_baselines/debug'
        debug_img_name = img_file.split('/')[-1]
        cv2.imwrite('{}/{}'.format(debug_path, debug_img_name), img)
        cv2.imwrite('{}/{}_label.png'.format(debug_path, debug_img_name[:-4]), mask)
        box_overlay = img.copy()
        for box in boxes:
            cv2.rectangle(box_overlay, (box[1], box[0]), (box[3], box[2]), (0, 0, 255), 3)
        cv2.imwrite('{}/{}_boxes.png'.format(debug_path, debug_img_name[:-4]), box_overlay)
        #print("len ids ", len(ids), " len boxes", len(boxes), " len unique mask ", len(np.unique(mask)))
        '''
        img = torch.from_numpy(img).permute(2, 0, 1).contiguous()
        mask = np.expand_dims(mask, axis=0)
        mask = torch.from_numpy(mask)
        boxes = torch.from_numpy(boxes)
        ids = torch.from_numpy(ids)

        if self.img_normalize:
            img = (img - self.mean) / self.std

        # torch.nn.utils.rnn.pad_sequence needs non-empty sequence
        '''
        if len(boxes) == 0:
            boxes = torch.Tensor([[0, 0, 0, 0]])
            ids = torch.Tensor([0])
        '''

        # Create dict for target
        target = {}
        target["boxes"] = boxes
        target["masks"] = mask
        target["labels"] = ids
        print("max in mask ", torch.max(torch.unique(mask)))
        print("max in labels ", torch.max(torch.unique(ids)))

        return img, target, scene

    '''
    @staticmethod
    def collate_fn(batch):
        img, mask, boxes, ids, scene = zip(*batch)
        return (
            torch.stack(img, 0),
            torch.stack(mask, 0),
            torch.nn.utils.rnn.pad_sequence(boxes, batch_first=True, padding_value=-1.0),
            torch.nn.utils.rnn.pad_sequence(ids, batch_first=True, padding_value=-1.0),
            scene
        )
    '''
    @staticmethod
    def collate_fn(batch):
        return tuple(zip(*batch))

