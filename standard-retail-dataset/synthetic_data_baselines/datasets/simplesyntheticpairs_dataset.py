import os
import cv2
import json
import time
import random
import imageio
import collections
import numpy as np

import torch

from synthetic_data_baselines.datasets.base_dataset import BaseDataset
from utils.constants import Action
from datasets.augment import (
    add_noise,
    box_augment,
    flip_lr,
    flip_pair,
    mask2bbox,
)


class SimpleSyntheticPairsDataSet(BaseDataset):
    """Dataset of synthetic image pairs and change masks from synthetic images generated by blender
    Using official splits from imagesets dir.
    """

    DELIM_CHAR = ["-", "_"]
    IMG_FILE1 = "0.png"  # _1.png
    IMG_FILE2 = "1.png"  # _2.png
    LABEL_FILE = "_label.png"
    BBOX_FILE = "_bbox.json"

    def __init__(self, configuration):
        self.root = configuration["root"]
        self.split = configuration["mode"]
        assert self.split in ["train", "val", "test"]
        self.files = collections.defaultdict(list)
        self.img_normalize = configuration["normalize"]
        self.crops = configuration["crop"]
        self.resize = configuration["resize"]
        self.spatial_resolution = configuration["spatial_resolution"]

        if not os.path.exists('/root/.imageio/freeimage'):
            imageio.plugins.freeimage.download()

        # For overfitting always use first 4 samples
        if configuration["overfit"]:
            print("Overfitting to 4 samples")
            with open(os.path.join('/app/synthetic_data_baselines/imagesets/train.txt'), 'r') as imgsetFile:
                examples_png = imgsetFile.readlines()
            examples_png = ["/app/renders_multicam_diff_1/"+e[:-1]+"_change-0.png" for e in examples_png]
            examples_png.sort()
            examples_png = examples_png[:4]
        else:
            with open(os.path.join('/app/synthetic_data_baselines/imagesets', self.split+'.txt'), 'r') as imgsetFile:
                examples_png = imgsetFile.readlines()
            examples_png = ["/app/renders_multicam_diff_1/"+e[:-1]+"_change-0.png" for e in examples_png]

        
        for im_path in examples_png:
            label_file = im_path.replace("_change-0.png", "-label.png")
            random_file = im_path.replace(".png", "-randommats.png")
            box_file = im_path.replace("_change-0.png", "-boxes.json")
            depth_file = im_path.replace(".png", "-depth0001.exr")

            im_path2 = im_path.replace("change-0", "change-1")
            random_file2 = random_file.replace("change-0", "change-1")
            depth_file2 = im_path2.replace(".png", "-depth0001.exr")

            if (
                os.path.exists(label_file)
                and os.path.exists(box_file)
                and os.path.exists(im_path2)
                and os.path.exists(random_file2)
            ):
                self.files[self.split].append(
                    {
                        "img1": im_path,
                        "img2": im_path2,
                        "label": label_file,
                        "bbox": box_file,
                        "depth1": depth_file,
                        "depth2": depth_file2
                    }
                )


        self.mean = torch.tensor([0.406 * 255, 0.456 * 255, 0.485 * 255]).view(3, 1, 1)
        self.std = torch.tensor([0.225 * 255, 0.224 * 255, 0.229 * 255]).view(3, 1, 1)
        self.mean_depth = torch.tensor(7.7304)
        self.depth_std = torch.tensor(1.4214)

        self.res_x = self.spatial_resolution[0]
        self.res_y = self.spatial_resolution[1]

        self.aug = configuration["augment"]

        self.augment_params = {
            "box_augment_params": {
                "max_boxes": 3,
                "min_height_mult": 0.1,
                "max_height_mult": 0.5,
                "min_width_mult": 0.1,
                "max_width_mult": 0.3,
                "sat_prob": 0.5,
                "sat_min": 0.5,
                "sat_max": 1.5,
                "brightness_prob": 0.5,
                "brightness_min": 0.5,
                "brightness_max": 1.5,
            },
            "noise_params": {
                "qe_low": 0.65,
                "qe_high": 0.72,
                "bit_depth": 8,
                "baseline": 0,
                "sensitivity_low": 1.2,
                "sensitivity_high": 1.7,
                "dark_noise_low": 2.5,
                "dark_noise_high": 3.5,
            },
            "flip_pair": 0.0,
            "flip_lr": 0.5,
        }


    def __len__(self):
        return len(self.files[self.split])

    def augment(
        self, img1, img2, label, bboxes, depth1, depth2,
    ):
        img1, img2 = add_noise(img1, img2, self.augment_params["noise_params"])
        img1, img2 = box_augment(img1, img2, self.augment_params["box_augment_params"])
        if random.random() < self.augment_params["flip_pair"]:
            img1, img2, bboxes, depth1, depth2 = flip_pair(img1, img2, bboxes, depth1, depth2)
        if random.random() < self.augment_params["flip_lr"]:
            img1, img2, label, bboxes, depth1, depth2 = flip_lr(img1, img2, label, bboxes, depth1, depth2)
        return img1, img2, label, bboxes, depth1, depth2

    def resize_imgs(
        self, img1, img2, label, depth1, depth2,
    ):
        img1 = cv2.resize(img1, (self.res_x, self.res_y))
        img2 = cv2.resize(img2, (self.res_x, self.res_y))
        depth1 = cv2.resize(depth1, (self.res_x, self.res_y))
        depth2 = cv2.resize(depth2, (self.res_x, self.res_y))
        label = cv2.resize(label, (self.res_x, self.res_y), interpolation=cv2.INTER_NEAREST)
        return img1, img2, label, depth1, depth2

    def crop(
        self, img1, img2, label, bboxes, depth1, depth2
    ):
        height, width = img1.shape[:2]

        bbox = mask2bbox(label)
        if bbox is not None:
            change_rmin, change_cmin, change_rmax, change_cmax = bbox
        else:
            change_rmin, change_cmin, change_rmax, change_cmax = 0, 0, height, width

        bbheight = change_rmax - change_rmin
        bbwidth = change_cmax - change_cmin

        if bbheight < self.res_y:
            rmin = max(change_rmin - self.res_y + bbheight, 0)
            rmax = change_rmin
        else:
            rmin = change_rmin
            rmax = change_rmin - self.res_y + bbheight
        if bbwidth < self.res_x:
            cmin = max(change_cmin - self.res_x + bbwidth, 0)
            cmax = change_cmin
        else:
            cmin = change_cmin
            cmax = change_cmin - self.res_x + bbwidth

        x = random.randint(cmin, cmax)
        y = random.randint(rmin, rmax)
        x = min(x, width - self.res_x - 1)
        y = min(y, height - self.res_y - 1)
        ymax = y + self.res_y
        xmax = x + self.res_x
        img1 = img1[y:ymax, x:xmax, :]
        img2 = img2[y:ymax, x:xmax, :]
        label = label[y:ymax, x:xmax]
        depth1 = depth1[y:ymax, x:xmax]
        depth2 = depth2[y:ymax, x:xmax]

        bboxes[:, :4] *= np.array([height, width, height, width])
        ya = np.fmax(bboxes[:, 0], y)
        xa = np.fmax(bboxes[:, 1], x)
        yb = np.fmin(bboxes[:, 2], ymax)
        xb = np.fmin(bboxes[:, 3], xmax)

        inds = np.logical_and((xb - xa) > 0, (yb - ya) > 0)
        xa, xb, ya, yb = xa[inds], xb[inds], ya[inds], yb[inds]
        bboxes = bboxes[inds, :]
        bboxes[:, 0] = ya - y
        bboxes[:, 1] = xa - x
        bboxes[:, 2] = yb - y
        bboxes[:, 3] = xb - x
        bboxes[:, :4] /= np.array([self.res_y, self.res_x, self.res_y, self.res_x])

        return img1, img2, label, bboxes, depth1, depth2

    def get_store(self, index: int):
        store = os.path.basename(self.files[self.split][index]["img1"]).split("_")[0]
        return store

    def __getitem__(self, index: int):
        datafiles = self.files[self.split][index]

        # Images
        img_file1, img_file2 = datafiles["img1"], datafiles["img2"]
        img1, img2 = (
            cv2.imread(img_file1),
            cv2.imread(img_file2),
        )

        # Label
        label = cv2.imread(datafiles["label"])
        removed = label[:, :, 0]
        added = np.where(label[:, :, 1], Action.ADDED.value, 0)
        shifted = np.where(label[:, :, 2], Action.SHIFTED.value, 0)
        # added gets priority (we need to break the tie), then removed, then shifts
        label = np.where(added, added, np.where(removed, removed, shifted)).astype(np.uint8)

        # Bounding Boxes
        with open(datafiles["bbox"]) as f:
            bboxes = json.load(f)
        output_boxes = bboxes["change_boxes"]
        # output_boxes.extend(
        #    random.sample(bboxes["no_change_boxes"], min(5, len(bboxes["no_change_boxes"])))
        # )
        output_boxes.extend(bboxes["no_change_boxes"][: min(5, len(bboxes["no_change_boxes"]))])
        bboxes = np.array(output_boxes).astype(np.float32)

        # Depth
        t0 = time.time()
        depth1, depth2 = (
            imageio.imread(datafiles["depth1"], format="EXR-FI"),
            imageio.imread(datafiles["depth2"], format="EXR-FI")
        )
        t1 = time.time()
        # Replace 'infinite' depth with max
        depth1[depth1>100] = -1
        depth1[depth1==-1] = np.max(depth1)
        depth2[depth2>100] = -1
        depth2[depth2==-1] = np.max(depth2)
        t2 = time.time()
        '''
        # Load camera intrinsics/extrinsics
        cam_data = json.load(open(datafiles["img1"].replace(".png", "_label.json"), "r"))
        t3 = time.time()
        cam_pos = cam_data['cam_params']['blender_data']['cam_position']
        R = np.array(cam_data['cam_params']['extrinsic'])[:3, :3]
        R[1:3] *= -1
        t = - R.dot(cam_pos)
        K = np.array(cam_data['cam_params']['intrinsic'])
        K[1][1] = K[0][0]
        intrinsic_matrix = K
        inverse_intrinsic = np.linalg.inv(intrinsic_matrix)
        t4 = time.time()
        # Convert depth from camera to absolute depth
        u_v_scale_factors1 = np.ones_like(depth1)
        for u in range(depth1.shape[1]):
            for v in range(depth1.shape[0]):
                u_v_scale_factors1[v, u] = np.linalg.norm(inverse_intrinsic.dot([u, v, 1]))
        t5 = time.time()
        depth_image_z_coord = depth1
        depth1 = depth1 * 1.0/u_v_scale_factors1
                
        u_v_scale_factors2 = np.ones_like(depth2)
        for u in range(depth2.shape[1]):
            for v in range(depth2.shape[0]):
                u_v_scale_factors2[v, u] = np.linalg.norm(inverse_intrinsic.dot([u, v, 1]))
        t6 = time.time()
        depth_image_z_coord = depth2
        depth2 = depth2 * 1.0/u_v_scale_factors2
        #print("Time for 1 ", t1 - t0, " 2 ", t2-t1, " 3 ", t3 - t2, " 4 ", t4-t3, " 5 ", t5 -t4, " 6 ", t6-t5)
        '''
        if self.crops and not self.resize:
            img1, img2, label, bboxes, depth1, depth2 = self.crop(img1, img2, label, bboxes, depth1, depth2)
        elif self.resize and not self.crops:
            img1, img2, label, depth1, depth2 = self.resize_imgs(img1, img2, label, depth1, depth2)
        elif self.crops and self.resize:
            if random.random() > 0.5:
                img1, img2, label, depth1, depth2 = self.resize_imgs(img1, img2, label, depth1, depth2)
            else:
                img1, img2, label, bboxes, depth1, depth2 = self.crop(img1, img2, label, bboxes, depth1, depth2)
        if self.aug:
            img1, img2, label, bboxes, depth1, depth2 = self.augment(img1, img2, label, bboxes, depth1, depth2)

        # Debug: save images and labels
        #tmp_name = ''.join(random.choices(string.ascii_uppercase + string.digits, k=5))
        #cv2.imwrite('debug/{}_img1.jpg'.format(tmp_name), img1)
        #cv2.imwrite('debug/{}_img2.jpg'.format(tmp_name), img2)
        #cv2.imwrite('debug/{}_label.jpg'.format(tmp_name), label)
        #imageio.show_formats()
        #imageio.imwrite('debug/{}_depth1.exr'.format(tmp_name), depth1.astype("float32"), format="EXR-FI")
        #imageio.imwrite('debug/{}_depth2.exr'.format(tmp_name), depth2.astype("float32"), format="EXR-FI")
        
        img1 = torch.from_numpy(img1).permute(2, 0, 1).contiguous()
        img2 = torch.from_numpy(img2).permute(2, 0, 1).contiguous()
        label = torch.from_numpy(label)
        depth1 = torch.from_numpy(depth1).contiguous()
        depth2 = torch.from_numpy(depth2).contiguous()
        depth1 = torch.unsqueeze(depth1, dim=0)
        depth2 = torch.unsqueeze(depth2, dim=0)

        bboxes[:, 0] = (bboxes[:, 2] + bboxes[:, 0]) / 2
        bboxes[:, 1] = (bboxes[:, 3] + bboxes[:, 1]) / 2
        bboxes[:, 2] = (bboxes[:, 2] - bboxes[:, 0]) * 2
        bboxes[:, 3] = (bboxes[:, 3] - bboxes[:, 1]) * 2
        classes = bboxes[0:4, 4].astype(np.int)
        bboxes = bboxes[0:4, :4]
        bboxes[:, 0:2] -= 0.5
        bboxes = torch.from_numpy(bboxes)
        classes = torch.from_numpy(classes)

        if self.img_normalize:
            img1 = (img1 - self.mean) / self.std
            img2 = (img2 - self.mean) / self.std
            depth1 = (depth1 - self.mean_depth) / self.depth_std
            depth2 = (depth2 - self.mean_depth) / self.depth_std

        return img1, img2, label, bboxes, classes, '_'.join(datafiles["img1"].split('/')[-1].split('_')[:3]), depth1, depth2

    @staticmethod
    def collate_fn(batch):
        img1, img2, label, bboxes, classes, scene, depth1, depth2 = zip(*batch)
        return (
            torch.stack(img1, 0),
            torch.stack(img2, 0),
            torch.stack(label, 0),
            torch.nn.utils.rnn.pad_sequence(bboxes, batch_first=True, padding_value=-1.0),
            torch.nn.utils.rnn.pad_sequence(classes, batch_first=True, padding_value=4),
            scene,
            torch.stack(depth1, 0),
            torch.stack(depth2, 0)
        )

